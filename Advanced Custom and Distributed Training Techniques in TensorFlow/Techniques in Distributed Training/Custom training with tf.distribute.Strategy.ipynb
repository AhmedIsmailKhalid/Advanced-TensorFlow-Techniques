{"cells":[{"cell_type":"markdown","metadata":{"id":"jYysdyb-CaWM"},"source":["# Custom training with tf.distribute.Strategy\n","\n","In this notebook, we will implement a distribution strategy to train on the [Oxford Flowers 102](https://www.tensorflow.org/datasets/catalog/oxford_flowers102) dataset. Distribution strategies enable training across multiple devices, although we'll start with a single device setup. The syntax used here will also be applicable for multi-device environments"]},{"cell_type":"markdown","metadata":{"id":"hUwJCVUPnW4J"},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dzLKpmZICaWN"},"outputs":[],"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","\n","# Helper libraries\n","import numpy as np\n","import os\n","from tqdm import tqdm"]},{"cell_type":"markdown","metadata":{"id":"MM6W__qraV55"},"source":["## Download the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7NsM-Bma5wNw"},"outputs":[],"source":["import tensorflow_datasets as tfds\n","tfds.disable_progress_bar()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7MqDQO0KCaWS"},"outputs":[],"source":["splits = ['train[:80%]', 'train[80%:90%]', 'train[90%:]']\n","\n","(train_examples, validation_examples, test_examples), info = tfds.load('oxford_flowers102', with_info=True, as_supervised=True, split = splits, data_dir='data/')\n","\n","num_examples = info.splits['train'].num_examples\n","num_classes = info.features['label'].num_classes"]},{"cell_type":"markdown","metadata":{"id":"4AXoHhrsbdF3"},"source":["## Create a strategy to distribute the variables and the graph"]},{"cell_type":"markdown","metadata":{"id":"5mVuLZhbem8d"},"source":["The `tf.distribute.MirroredStrategy` is a TensorFlow strategy for synchronous training across multiple GPUs on the same machine. This strategy is designed to provide efficient distributed training with the following key steps:\n","\n","1. **Replication of Variables and Graph:** It begins by replicating all the variables and the model graph across multiple devices or replicas. This setup ensures that each device has a copy of the model to work with.\n","\n","2. **Distribution of Input:** The input data is evenly distributed or split among the replicas. Each replica receives a portion of the input data, ensuring parallel processing that maximizes the utilization of all available GPUs.\n","\n","3. **Local Computation:** Each replica independently computes the outputs, losses, and gradients based on the subset of data it has received. This parallel computation allows for efficient handling of large datasets by dividing the workload.\n","\n","4. **Synchronization of Gradients:** Once all replicas have computed their gradients, these gradients are synchronized across all replicas. Typically, this involves summing the gradients from all replicas to ensure that all replicas contribute equally to the learning process.\n","\n","5. **Update of Variables:** After synchronization, the updates calculated from the combined gradients are applied uniformly across all replicas. This ensures that all copies of the variables are updated consistently, keeping the model's state synchronized across all devices.\n","\n","The `tf.distribute.MirroredStrategy` effectively leverages multiple GPUs to speed up training by parallelizing the computation and gradient synchronization, making it a popular choice for optimizing training performance on multi-GPU setups."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F2VeZUWUj5S4"},"outputs":[],"source":["# If the list of devices is not specified in the\n","# `tf.distribute.MirroredStrategy` constructor, it will be auto-detected.\n","strategy = tf.distribute.MirroredStrategy()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1718155794927,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"},"user_tz":240},"id":"ZngeM_2o0_JO","outputId":"461d1e83-a9bb-481b-84f6-fd85a6eaf109"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of devices: 1\n"]}],"source":["print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"]},{"cell_type":"markdown","metadata":{"id":"k53F5I_IiGyI"},"source":["## Setup input pipeline"]},{"cell_type":"markdown","metadata":{"id":"0Qb6nDgxiN_n"},"source":["Let's set up some essential constants for our training process, including the buffer size, number of epochs, and image size. These parameters are crucial for managing how our data is processed and ensuring that our training sessions run smoothly and efficiently."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1718155794927,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"},"user_tz":240},"id":"jwJtsCQhHK-E","outputId":"8346da8a-f43a-4f64-ea63-36277c834a27"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using data/resnet_50_feature_vector with input size (224, 224)\n"]}],"source":["BUFFER_SIZE = num_examples\n","EPOCHS = 10\n","pixels = 224\n","MODULE_HANDLE = 'data/resnet_50_feature_vector'\n","IMAGE_SIZE = (pixels, pixels)\n","print(\"Using {} with input size {}\".format(MODULE_HANDLE, IMAGE_SIZE))"]},{"cell_type":"markdown","metadata":{"id":"rWUl3kUk8D5d"},"source":["We will define a function to properly format our images. This function will resize each image to a uniform dimension and scale the pixel values to a [0,1] range. Ensuring that all images are uniformly processed is vital for maintaining consistent model training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RHGFit478BWD"},"outputs":[],"source":["def format_image(image, label):\n","    image = tf.image.resize(image, IMAGE_SIZE) / 255.0\n","    return  image, label"]},{"cell_type":"markdown","metadata":{"id":"FeuHAg5enW4Q"},"source":["## Set the global batch size\n","\n","Now, we'll set the `GLOBAL_BATCH_SIZE` using the function we've just defined. This parameter is essential for dictating how much data our model processes in one go, balancing speed and memory usage optimally"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MnimHdbinW4Q"},"outputs":[],"source":["def set_global_batch_size(batch_size_per_replica, strategy):\n","    '''\n","    Args:\n","        batch_size_per_replica (int) - batch size per replica\n","        strategy (tf.distribute.Strategy) - distribution strategy\n","    '''\n","\n","    # Set the global batch size\n","    global_batch_size = batch_size_per_replica * strategy.num_replicas_in_sync\n","\n","    return global_batch_size"]},{"cell_type":"markdown","metadata":{"id":"RULVIAqWnW4R"},"source":["After setting the global batch size, we should expect an output that confirms the size has been set correctly. For instance, an output of `64` would indicate that our batch size parameter is properly configured."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1718155794927,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"},"user_tz":240},"id":"4bhmB1X3nW4R","outputId":"236a8c6c-aee7-4ca4-b05d-f248a1a00948"},"outputs":[{"name":"stdout","output_type":"stream","text":["64\n"]}],"source":["BATCH_SIZE_PER_REPLICA = 64\n","GLOBAL_BATCH_SIZE = set_global_batch_size(BATCH_SIZE_PER_REPLICA, strategy)\n","\n","print(GLOBAL_BATCH_SIZE)"]},{"cell_type":"markdown","metadata":{"id":"J7fj3GskHC8g"},"source":["We will also use the global batch size to create batches for training, validation and test sets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WYrMNNDhAvVl"},"outputs":[],"source":["train_batches = train_examples.shuffle(num_examples // 4).map(format_image).batch(BATCH_SIZE_PER_REPLICA).prefetch(1)\n","validation_batches = validation_examples.map(format_image).batch(BATCH_SIZE_PER_REPLICA).prefetch(1)\n","test_batches = test_examples.map(format_image).batch(1)"]},{"cell_type":"markdown","metadata":{"id":"Th0kjcoFnW4S"},"source":["## Define the distributed datasets\n","\n","We'll create the distributed datasets for both training and validation by utilizing the `experimental_distribute_dataset()` method from the [Strategy](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) class. This method efficiently distributes the batches across the available devices under the specified strategy, ensuring optimal data handling and computation distribution. By applying this method to our training and validation batches, we enhance our model's ability to process data efficiently across different compute resources, which is crucial for achieving faster and more scalable training. This step is fundamental in leveraging the full potential of distributed computing in our model's training process."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V8QjSh8InW4S"},"outputs":[],"source":["def distribute_datasets(strategy, train_batches, validation_batches, test_batches):\n","\n","    train_dist_dataset = strategy.experimental_distribute_dataset(train_batches)\n","    val_dist_dataset = strategy.experimental_distribute_dataset(validation_batches)\n","    test_dist_dataset = strategy.experimental_distribute_dataset(test_batches)\n","\n","    return train_dist_dataset, val_dist_dataset, test_dist_dataset"]},{"cell_type":"markdown","metadata":{"id":"IQEpaqWFnW4T"},"source":["Let's use the function we just defined to create distributed datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wa68y8OfnW4T"},"outputs":[],"source":["train_dist_dataset, val_dist_dataset, test_dist_dataset = distribute_datasets(strategy, train_batches, validation_batches, test_batches)"]},{"cell_type":"markdown","metadata":{"id":"PE6ITNJXnW4T"},"source":["It is a good idea to explore the `type` of our created datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1718155794927,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"},"user_tz":240},"id":"rhZ8_yzMnW4U","outputId":"40fe0096-5295-42e0-b5b8-aafbc9c41a73"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'tensorflow.python.distribute.input_lib.DistributedDataset'>\n","<class 'tensorflow.python.distribute.input_lib.DistributedDataset'>\n","<class 'tensorflow.python.distribute.input_lib.DistributedDataset'>\n"]}],"source":["print(type(train_dist_dataset))\n","print(type(val_dist_dataset))\n","print(type(test_dist_dataset))"]},{"cell_type":"markdown","metadata":{"id":"SGQaZeIAnW4U"},"source":["Let's take a quick look at a single batch from `train_dist_dataset` to see what our distributed dataset looks like"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":985,"status":"ok","timestamp":1718155795910,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"},"user_tz":240},"id":"TpsjB8yHnW4U","outputId":"bf0df836-91bd-4f07-b345-6b8eddd4195e"},"outputs":[{"name":"stdout","output_type":"stream","text":["x is a tuple that contains 2 values \n","x[0] contains the features, and has shape (64, 224, 224, 3)\n","  so it has 64 examples in the batch, each is an image that is (224, 224, 3)\n","x[1] contains the labels, and has shape (64,)\n"]}],"source":["# Take a look at a single batch from the train_dist_dataset\n","x = iter(train_dist_dataset).get_next()\n","\n","print(f\"x is a tuple that contains {len(x)} values \")\n","print(f\"x[0] contains the features, and has shape {x[0].shape}\")\n","print(f\"  so it has {x[0].shape[0]} examples in the batch, each is an image that is {x[0].shape[1:]}\")\n","print(f\"x[1] contains the labels, and has shape {x[1].shape}\")"]},{"cell_type":"markdown","metadata":{"id":"bAXAo_wWbWSb"},"source":["## Create the model\n","\n","We'll define ResNetModel as a subclass of tf.keras.Model using the Model Subclassing API. This approach provides flexibility in defining custom behaviors and layers for our model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ODch-OFCaW4"},"outputs":[],"source":["MODULE_HANDLE = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/5\"\n","\n","class ResNetModel(tf.keras.Model):\n","    def __init__(self, classes):\n","        super(ResNetModel, self).__init__()\n","        self._feature_extractor = hub.KerasLayer(MODULE_HANDLE,\n","                                                 trainable=False)\n","        self._classifier = tf.keras.layers.Dense(classes, activation='softmax')\n","\n","    def call(self, inputs):\n","        x = self._feature_extractor(inputs)\n","        x = self._classifier(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"OWBlR3IpnW4V"},"source":["We will create a directory to store checkpoints during training. Checkpoints capture the model's weights at various stages, allowing for recovery or starting subsequent training from a known state."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9iagoTBfijUz"},"outputs":[],"source":["# Create a checkpoint directory to store the checkpoints.\n","checkpoint_dir = './training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")"]},{"cell_type":"markdown","metadata":{"id":"e-wlFFZbP33n"},"source":["## Define the loss function\n","\n","Within the `strategy.scope()`, we'll define the `loss_object` for later use in test set evaluations and compute_loss for calculating average loss during training. This scoping ensures that our loss computations are compatible with the distributed strategy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R144Wci782ix"},"outputs":[],"source":["  with strategy.scope():\n","    # Set reduction to `NONE` so we can do the reduction afterwards and divide by global batch size.\n","    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","        reduction=tf.keras.losses.Reduction.NONE)\n","    # or loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n","    def compute_loss(labels, predictions):\n","        per_example_loss = loss_object(labels, predictions)\n","        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n","\n","    test_loss = tf.keras.metrics.Mean(name='test_loss')"]},{"cell_type":"markdown","metadata":{"id":"w8y54-o9T2Ni"},"source":["## Define the metrics to track loss and accuracy\n","\n","To define the metrics for trackking loss and accuracy for both training and testing, we'll utilize `.result()` to retrieve accumulated statistics, providing insights into the model's performance throughout the training and evaluation phases."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zt3AHb46Tr3w"},"outputs":[],"source":["with strategy.scope():\n","    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n","        name='train_accuracy')\n","    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n","        name='test_accuracy')"]},{"cell_type":"markdown","metadata":{"id":"iuKuNXPORfqJ"},"source":["## Instantiate the model, optimizer, and checkpoints\n","\n","Also within the `strategy.scope()`, we will instantiate the ResNetModel specifying the number of classes, and create an instance of the Adam optimizer. Set up a checkpoint mechanism for the model and its optimizer to ensure training progress is saved."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OrMmakq5EqeQ"},"outputs":[],"source":["# Model and optimizer must be created under `strategy.scope`.\n","with strategy.scope():\n","    model = ResNetModel(classes=num_classes)\n","    optimizer = tf.keras.optimizers.Adam()\n","    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)"]},{"cell_type":"markdown","metadata":{"id":"cwlQ8slynW4W"},"source":["## Training Loop\n","\n","We will define a regular training step and test step, which can function without a distributed strategy. We will then use `strategy.run` to apply these functions in a distributed manner.\n","- Notice that we will define `train_step` and `test_step` inside another function `train_test_step_fns`, which will then return these two functions.\n","\n","### Define train_step\n","Within the strategy's scope, we will define `train_step(inputs)`\n","- `inputs` will be a tuple containing `(images, labels)`.\n","- Create a gradient tape block.\n","- Within the gradient tape block:\n","  - Call the model, passing in the images and setting training to `True` (complete this part).\n","  - Call the `compute_loss` function (defined earlier) to compute the training loss (complete this part).\n","  - Use the gradient tape to calculate the gradients.\n","  - Use the optimizer to update the weights using the gradients.\n","\n","### Define test_step\n","Also within the strategy's scope, we will define `test_step(inputs)`\n","- `inputs` is a tuple containing `(images, labels)`.\n","  - Call the model, passing in the images and setting training to `False`, because the model is not going to train on the test data. (complete this part).\n","  - Use the `loss_object`, which will compute the test loss. Check `compute_loss`, defined earlier, to see what parameters to pass into `loss_object`. (complete this part).\n","  - Next, update `test_loss` (the running test loss) with the `t_loss` (the loss for the current batch).\n","  - Also update the `test_accuracy`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zUQ_nAP1MtA9"},"outputs":[],"source":["def train_test_step_fns(strategy, model, compute_loss, optimizer, train_accuracy, loss_object, test_loss, test_accuracy):\n","    with strategy.scope():\n","        def train_step(inputs):\n","            images, labels = inputs\n","\n","            with tf.GradientTape() as tape:\n","                predictions = model(images, training=True)\n","                loss = compute_loss(labels, predictions)\n","\n","            gradients = tape.gradient(loss, model.trainable_variables)\n","            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","\n","            train_accuracy.update_state(labels, predictions)\n","            return loss\n","\n","        def test_step(inputs):\n","            images, labels = inputs\n","\n","            predictions = model(images, training=False)\n","            t_loss = compute_loss(labels, predictions)\n","\n","            test_loss.update_state(t_loss)\n","            test_accuracy.update_state(labels, predictions)\n","\n","        return train_step, test_step"]},{"cell_type":"markdown","metadata":{"id":"iHOOTe90nW4Y"},"source":["We will use the train_test_step_fns function to generate the train_step and test_step functions. These functions are initially designed for a non-distributed setup but are flexible enough to be adapted for distributed training and testing using TensorFlow's distribution strategies."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fM-9lGWinW4Y"},"outputs":[],"source":["train_step, test_step = train_test_step_fns(strategy, model, compute_loss, optimizer, train_accuracy, loss_object, test_loss, test_accuracy)"]},{"cell_type":"markdown","metadata":{"id":"U-82osZpnW4Y"},"source":["## Distributed training and testing (please complete this section)\n","\n","`Distributed Train Step`\n","To apply the train_step in a distributed environment, we will utilize the strategy.run method. Here's how we will set it up:\n","\n","*   **Utilizing strategy.run:** We will call the run function of the strategy, passing in our previously defined train_step as the function argument, along with the dataset inputs.\n","*   **Function Call Format:** The run function is formatted as run(fn, args=()), where fn is the function to execute (in this case, train_step) and args takes a tuple of the dataset inputs necessary for that function.\n","\n","\n","`Distributed Test Step`\n","Similarly, for the testing phase:\n","\n","* **Setting up distributed_test_step:** We will use the strategy.run method again, this time passing in the test_step along with its required dataset inputs.\n","* **Execution:** This method will distribute the test step across the available devices, allowing for efficient evaluation of the model on the test dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1718155798495,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"},"user_tz":240},"id":"WJXCknQjnW4Z","outputId":"8bcd7193-a7d3-428b-b6ca-8eeb8e3dca99"},"outputs":[{"name":"stdout","output_type":"stream","text":["When passing in args=list_of_inputs:\n","number of arguments passed is 2\n","\n","When passing in args=(list_of_inputs)\n","number of arguments passed is 2\n","\n","When passing in args=(list_of_inputs,)\n","number of arguments passed is 1\n"]}],"source":["# See various ways of passing in the inputs\n","\n","def fun1(args=()):\n","    print(f\"number of arguments passed is {len(args)}\")\n","\n","\n","list_of_inputs = [1,2]\n","print(\"When passing in args=list_of_inputs:\")\n","fun1(args=list_of_inputs)\n","print()\n","print(\"When passing in args=(list_of_inputs)\")\n","fun1(args=(list_of_inputs))\n","print()\n","print(\"When passing in args=(list_of_inputs,)\")\n","fun1(args=(list_of_inputs,))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F2jC72QZnW4Z"},"outputs":[],"source":["def distributed_train_test_step_fns(strategy, train_step, test_step, model, compute_loss, optimizer, train_accuracy, loss_object, test_loss, test_accuracy):\n","    with strategy.scope():\n","        @tf.function\n","        def distributed_train_step(dataset_inputs):\n","            per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))\n","            return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n","                                   axis=None)\n","\n","        @tf.function\n","        def distributed_test_step(dataset_inputs):\n","            return strategy.run(test_step, args=(dataset_inputs,))\n","\n","        return distributed_train_step, distributed_test_step"]},{"cell_type":"markdown","metadata":{"id":"gPHnh1WpnW4Z"},"source":["Once these functions are defined within the scope of our distribution strategy, we will call the train_test_step_fns to retrieve the train_step and test_step. This call effectively links our training and testing functions with the distribution logic provided by TensorFlow, ensuring they are ready to handle data across multiple compute resources efficiently"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TD_dMQm6nW4Z"},"outputs":[],"source":["distributed_train_step, distributed_test_step = distributed_train_test_step_fns(strategy, train_step, test_step, model, compute_loss, optimizer, train_accuracy, loss_object, test_loss, test_accuracy)"]},{"cell_type":"markdown","metadata":{"id":"n0pAENzwnW4a"},"source":["## Run the distributed training in a loop\n","\n","To effectively train and evaluate our model using a distributed strategy across multiple epochs, we'll implement a structured approach within a for-loop. This loop will handle both training and testing phases, ensuring that our model learns from the training data and is accurately assessed using the test data at each epoch. Here's how we'll set it up:\n","\n","### Training and Testing in a Distributed Manner\n","\n","1. **Epoch Loop**:\n","   - Begin by looping through the desired number of epochs to train the model for multiple cycles over the dataset.\n","\n","2. **Training Phase**:\n","   - **Loop through Training Batches**: For each epoch, iterate through each batch of the distributed training dataset.\n","   - **Execute Training Step**: For each training batch, call the `distributed_train_step`. This function will apply the training step across all replicas and return the loss for that batch.\n","   - **Calculate Average Training Loss**: After completing all training batches for the epoch, calculate the average training loss to get a sense of how well the model is learning over time.\n","\n","3. **Testing Phase**:\n","   - **Loop through Test Batches**: Similar to training, loop through each batch of the distributed test set.\n","   - **Execute Test Step**: For each test batch, run the `distributed_test_step`. This step will update the test loss and test accuracy based on the outcomes of the model's predictions compared to the actual labels.\n","   - **Store Metrics**: Accumulate the test losses and accuracies to later calculate averages for the epoch.\n","\n","4. **Reporting**:\n","   - **Print Results**: At the end of each epoch, print out the epoch number, the average training loss, the training accuracy, the test loss, and the test accuracy. This step provides a checkpoint to monitor the model's performance and improvements epoch by epoch.\n","   - **Reset Metrics**: Before moving to the next epoch, reset the metrics for training and test losses and accuracies. This reset is crucial to ensure that the metrics for each epoch are calculated independently of previous epochs.\n","\n","5. **Repeat**:\n","   - This process repeats for the specified number of epochs, allowing the model to progressively learn and improve its predictive accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":73116,"status":"ok","timestamp":1718155871607,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"},"user_tz":240},"id":"gX975dMSNw0e","outputId":"0516b584-0131-482c-b7ce-58ab0e1ab828"},"outputs":[{"name":"stderr","output_type":"stream","text":["13it [00:32,  2.46s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 4.741341590881348, Accuracy: 4.779411792755127, Test Loss: 0.0638691708445549, Test Accuracy: 8.823529243469238\n"]},{"name":"stderr","output_type":"stream","text":["13it [00:02,  4.92it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, Loss: 2.8097712993621826, Accuracy: 42.52450942993164, Test Loss: 0.048645444214344025, Test Accuracy: 34.31372833251953\n"]},{"name":"stderr","output_type":"stream","text":["13it [00:02,  4.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, Loss: 1.6825740337371826, Accuracy: 78.06372833251953, Test Loss: 0.039348047226667404, Test Accuracy: 50.0\n"]},{"name":"stderr","output_type":"stream","text":["13it [00:02,  5.22it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, Loss: 1.0582751035690308, Accuracy: 91.17646789550781, Test Loss: 0.034339699894189835, Test Accuracy: 55.88235092163086\n"]},{"name":"stderr","output_type":"stream","text":["13it [00:02,  4.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, Loss: 0.7129174470901489, Accuracy: 95.3431396484375, Test Loss: 0.030914997681975365, Test Accuracy: 60.78431701660156\n"]},{"name":"stderr","output_type":"stream","text":["13it [00:02,  5.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6, Loss: 0.5220160484313965, Accuracy: 97.67156982421875, Test Loss: 0.028901422396302223, Test Accuracy: 64.70588684082031\n"]},{"name":"stderr","output_type":"stream","text":["13it [00:02,  5.24it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7, Loss: 0.3930404782295227, Accuracy: 99.14215850830078, Test Loss: 0.027198707684874535, Test Accuracy: 65.68627166748047\n"]},{"name":"stderr","output_type":"stream","text":["13it [00:02,  4.93it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8, Loss: 0.3086308240890503, Accuracy: 99.50980377197266, Test Loss: 0.026264455169439316, Test Accuracy: 66.66667175292969\n"]},{"name":"stderr","output_type":"stream","text":["13it [00:02,  5.37it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9, Loss: 0.24907761812210083, Accuracy: 99.75489807128906, Test Loss: 0.025534609332680702, Test Accuracy: 64.70588684082031\n"]},{"name":"stderr","output_type":"stream","text":["13it [00:02,  5.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10, Loss: 0.20324990153312683, Accuracy: 99.87745666503906, Test Loss: 0.025066642090678215, Test Accuracy: 66.66667175292969\n"]}],"source":["with strategy.scope():\n","    for epoch in range(EPOCHS):\n","        # TRAIN LOOP\n","        total_loss = 0.0\n","        num_batches = 0\n","        for x in tqdm(train_dist_dataset):\n","            total_loss += distributed_train_step(x)\n","            num_batches += 1\n","        train_loss = total_loss / num_batches\n","\n","        # TEST LOOP\n","        for x in test_dist_dataset:\n","            distributed_test_step(x)\n","\n","        template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, \"\n","                    \"Test Accuracy: {}\")\n","        print (template.format(epoch+1, train_loss,\n","                               train_accuracy.result()*100, test_loss.result(),\n","                               test_accuracy.result()*100))\n","\n","        test_loss.reset_states()\n","        train_accuracy.reset_states()\n","        test_accuracy.reset_states()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"coursera":{"schema_names":["TF3C2W4-1","TF3C2W4-2","TF3C2W4-3","TF3C2W4-4"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}
