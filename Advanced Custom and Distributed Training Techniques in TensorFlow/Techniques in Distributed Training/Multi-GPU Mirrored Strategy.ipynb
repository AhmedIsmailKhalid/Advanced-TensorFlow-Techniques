{"cells":[{"cell_type":"markdown","metadata":{"id":"5kJiUy1qfcHR"},"source":["# Multi-GPU Mirrored Strategy\n","\n","In this notebook, we'll delve into the basics of implementing [Multi-GPU Mirrored Strategy](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy).\n","\n","**Note:** To use MirroredStrategy(), we need to use GPU. It is possible that when running on GPU, there will only be 1 GPU device listed. This is not a problem as the purpose of this notebook is to explore and understand distributed strategies"]},{"cell_type":"markdown","metadata":{"id":"RMhCWLzlf-lP"},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IZqzf1GFgMbh"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import os"]},{"cell_type":"markdown","metadata":{"id":"lK08VhgKgNm8"},"source":["## Setup Distribution Strategy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8l08oKT0gP_h","outputId":"cb9e41fb-5a5f-41d4-a39a-2a017a79e783"},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n","INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n","Number of devices: 1\n"]}],"source":["# This notebook is imported and executed in Google Colab, and the GPU provided by Google Colab has minimum of 8 cores. If the GPU we are using has different number of cores, we need to adjust this\n","os.environ[\"TF_MIN_GPU_MULTIPROCESSOR_COUNT\"] = \"4\"\n","\n","# If the list of devices is not specified in the `tf.distribute.MirroredStrategy` constructor, it will be auto-detected.\n","# If we have *different* GPUs in the system, we probably have to set up cross_device_ops like this\n","strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n","print ('Number of devices: {}'.format(strategy.num_replicas_in_sync))"]},{"cell_type":"markdown","metadata":{"id":"1fNVYyBHgcJv"},"source":["## Prepare the Data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BlCtFL__gdQO","outputId":"fdc8f171-5487-45a3-e30a-fb2c5b8a8e13"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n","32768/29515 [=================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n","26427392/26421880 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n","8192/5148 [===============================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n","4423680/4422102 [==============================] - 0s 0us/step\n"]}],"source":["# Get the data\n","fashion_mnist = tf.keras.datasets.fashion_mnist\n","(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n","\n","# Adding a dimension to the array -> new shape == (28, 28, 1) as the first layer in our model is a convolutional # layer and it requires a 4D input (batch_size, height, width, channels).\n","# batch_size dimension will be added later on.\n","train_images = train_images[..., None]\n","test_images = test_images[..., None]\n","\n","# Normalize the images to [0, 1] range.\n","train_images = train_images / np.float32(255)\n","test_images = test_images / np.float32(255)\n","\n","# Batch the input data\n","BUFFER_SIZE = len(train_images)\n","BATCH_SIZE_PER_REPLICA = 64\n","GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n","\n","# Create Datasets from the batches\n","train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE)\n","test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE)\n","\n","# Create Distributed Datasets from the datasets\n","train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n","test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)"]},{"cell_type":"markdown","metadata":{"id":"YL0OZBhpgmX-"},"source":["## Define the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q5lOiEUCgpH7"},"outputs":[],"source":["# Create the model architecture\n","def create_model():\n","  model = tf.keras.Sequential([\n","      tf.keras.layers.Conv2D(32, 3, activation='relu'),\n","      tf.keras.layers.MaxPooling2D(),\n","      tf.keras.layers.Conv2D(64, 3, activation='relu'),\n","      tf.keras.layers.MaxPooling2D(),\n","      tf.keras.layers.Flatten(),\n","      tf.keras.layers.Dense(64, activation='relu'),\n","      tf.keras.layers.Dense(10)\n","    ])\n","  return model"]},{"cell_type":"markdown","metadata":{"id":"VV9U5gNwgqqj"},"source":["## Configure custom training\n","\n","Instead of using `model.compile()`, we'll opt for custom training, so let's encapsulate that within a strategy scope."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J-aJMgRTgvdd","outputId":"56071655-d3e8-4326-a6a4-b39220b27dbc"},"outputs":[{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n","INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"]}],"source":["# Instead of model.compile, we're going to do custom training, so let's do that\n","# within a strategy scope\n","with strategy.scope():\n","    # We will use sparse categorical crossentropy as always. But, instead of having the loss function manage the map reduce across GPUs for us, we'll do it ourselves with a simple algorithm.\n","    # Remember -- the map reduce is how the losses get aggregated\n","    # Setting reduction to `none` so we can do the reduction afterwards and divide byglobal batch size.\n","    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n","\n","    def compute_loss(labels, predictions):\n","        # Compute Loss uses the loss object to compute the loss\n","        # Notice that per_example_loss will have an entry per GPU so in this case there'll be 2 -- i.e. the loss for each replica\n","        per_example_loss = loss_object(labels, predictions)\n","\n","        # Note in particular that replica_0 isn't named in the weighted_loss -- the first is unnamed, the second is replica_1 etc\n","        print(per_example_loss)\n","        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n","\n","    # We'll just reduce by getting the average of the losses\n","    test_loss = tf.keras.metrics.Mean(name='test_loss')\n","\n","    # Accuracy on train and test will be SparseCategoricalAccuracy\n","    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n","    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n","\n","    # Optimizer will be Adam\n","    optimizer = tf.keras.optimizers.Adam()\n","\n","    # Create the model within the scope\n","    model = create_model()"]},{"cell_type":"markdown","metadata":{"id":"P4F-MI-Ygyad"},"source":["## Train and Test Steps Functions\n","\n","We'll define several functions to streamline the training process."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zcx6Fg81g1On"},"outputs":[],"source":["# `run` replicates the provided computation and runs it\n","\n","# With the distributed input.\n","@tf.function\n","def distributed_train_step(dataset_inputs):\n","  per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))\n","  #tf.print(per_replica_losses.values)\n","  return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n","\n","def train_step(inputs):\n","  images, labels = inputs\n","  with tf.GradientTape() as tape:\n","    predictions = model(images, training=True)\n","    loss = compute_loss(labels, predictions)\n","\n","  gradients = tape.gradient(loss, model.trainable_variables)\n","  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","\n","  train_accuracy.update_state(labels, predictions)\n","  return loss\n","\n","# Test Steps Functions\n","@tf.function\n","def distributed_test_step(dataset_inputs):\n","  return strategy.run(test_step, args=(dataset_inputs,))\n","\n","def test_step(inputs):\n","  images, labels = inputs\n","\n","  predictions = model(images, training=False)\n","  t_loss = loss_object(labels, predictions)\n","\n","  test_loss.update_state(t_loss)\n","  test_accuracy.update_state(labels, predictions)"]},{"cell_type":"markdown","metadata":{"id":"HxjOZ3-Eg6FP"},"source":["## Training Loop\n","\n","Now we can start training our model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k0VOdqKP2NEz","outputId":"cb8c6211-b913-48d4-a44a-0b97c82da3b7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tensor(\"sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(64,), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n","Tensor(\"sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(64,), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n","Tensor(\"sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(32,), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n","Epoch 1, Loss: 0.5053812265396118, Accuracy: 81.72333526611328, Test Loss: 0.3808790147304535, Test Accuracy: 86.36000061035156\n","Epoch 2, Loss: 0.3329238295555115, Accuracy: 88.02166748046875, Test Loss: 0.32122674584388733, Test Accuracy: 88.63999938964844\n","Epoch 3, Loss: 0.2873055338859558, Accuracy: 89.56000518798828, Test Loss: 0.3218331038951874, Test Accuracy: 88.34000396728516\n","Epoch 4, Loss: 0.2574699819087982, Accuracy: 90.5133285522461, Test Loss: 0.2930581271648407, Test Accuracy: 89.52999877929688\n","Epoch 5, Loss: 0.23423510789871216, Accuracy: 91.30999755859375, Test Loss: 0.2917958199977875, Test Accuracy: 89.33000183105469\n","Epoch 6, Loss: 0.21285584568977356, Accuracy: 92.1066665649414, Test Loss: 0.2735103368759155, Test Accuracy: 90.13999938964844\n","Epoch 7, Loss: 0.1978341042995453, Accuracy: 92.66166687011719, Test Loss: 0.2520335912704468, Test Accuracy: 91.02999877929688\n","Epoch 8, Loss: 0.17923995852470398, Accuracy: 93.32167053222656, Test Loss: 0.25569435954093933, Test Accuracy: 90.75\n","Epoch 9, Loss: 0.16436836123466492, Accuracy: 93.94499969482422, Test Loss: 0.273624062538147, Test Accuracy: 90.52000427246094\n","Epoch 10, Loss: 0.15223374962806702, Accuracy: 94.30166625976562, Test Loss: 0.26304712891578674, Test Accuracy: 90.88999938964844\n"]}],"source":["EPOCHS = 10\n","for epoch in range(EPOCHS):\n","  # Training\n","  total_loss = 0.0\n","  num_batches = 0\n","  for batch in train_dist_dataset:\n","    total_loss += distributed_train_step(batch)\n","    num_batches += 1\n","  train_loss = total_loss / num_batches\n","\n","  # Testing\n","  for batch in test_dist_dataset:\n","    distributed_test_step(batch)\n","\n","  template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, \" \"Test Accuracy: {}\")\n","\n","  print (template.format(epoch+1, train_loss, train_accuracy.result()*100, test_loss.result(), test_accuracy.result()*100))\n","\n","  test_loss.reset_states()\n","  train_accuracy.reset_states()\n","  test_accuracy.reset_states()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":0}
