{"cells":[{"cell_type":"markdown","metadata":{"id":"FrxFqa-WWL1q"},"source":["# Gradient Tape Basics\n","\n","In this notebook, we'll get familiar with TensorFlow's built-in API called Gradient Tape, which helps in performing automatic differentiation."]},{"cell_type":"markdown","metadata":{"id":"b72bwfB8WL1u"},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uQe_MWjNPQkR"},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"markdown","metadata":{"id":"sIZp9CMuWL1w"},"source":["## Basics of Gradient Tape\n","\n","Let's explore how we can use [tf.GradientTape()](https://www.tensorflow.org/api_docs/python/tf/GradientTape) to perform automatic differentiation."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":153,"status":"ok","timestamp":1717990759522,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"},"user_tz":240},"id":"57Vnn9iIPNh9","outputId":"33ed7e7a-ad95-4598-ffc4-a01cd73a9992"},"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor(\n","[[8. 8.]\n"," [8. 8.]], shape=(2, 2), dtype=float32)\n"]}],"source":["# Define a 2x2 array of 1's\n","x = tf.ones((2,2))\n","\n","with tf.GradientTape() as t:\n","    # Record the actions performed on tensor x with `watch`\n","    t.watch(x)\n","\n","    # Define y as the sum of the elements in x\n","    y =  tf.reduce_sum(x)\n","\n","    # Let z be the square of y\n","    z = tf.square(y)\n","\n","# Get the derivative of z wrt the original input tensor x\n","dz_dx = t.gradient(z, x)\n","\n","# Print our result\n","print(dz_dx)"]},{"cell_type":"markdown","metadata":{"id":"5P7Ts4hpWL1y"},"source":["### Gradient tape expiration\n","\n","By default, GradientTape is not persistent (`persistent=False`), meaning it expires after a single use. If multiple gradients need to be computed, this default setting will not suffice. To see this in action, set up gradient tape and calculate a gradient. Notice how the gradient tape will 'expire' after this calculation."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1717990759522,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"},"user_tz":240},"id":"9EaLq2IJWL1z","outputId":"20666457-2ecd-4702-a358-f74f17ba4641"},"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor(108.0, shape=(), dtype=float32)\n"]}],"source":["x = tf.constant(3.0)\n","\n","# Notice that persistent is False by default\n","with tf.GradientTape() as t:\n","    t.watch(x)\n","\n","    # y = x^2\n","    y = x * x\n","\n","    # z = y^2\n","    z = y * y\n","\n","# Compute dz/dx. 4 * x^3 at x = 3 --> 108.0\n","dz_dx = t.gradient(z, x)\n","print(dz_dx)"]},{"cell_type":"markdown","metadata":{"id":"KpDgaLwfWL10"},"source":["#### Gradient tape has expired\n","\n","Observe the result when attempting to calculate another gradient after the gradient tape has already been used once."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1717990759522,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"},"user_tz":240},"id":"yMvfUJyPWL10","outputId":"b468dadf-561a-432c-87fc-b376ba2b3efd"},"outputs":[{"name":"stdout","output_type":"stream","text":["The error message is:\n","A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)\n"]}],"source":["# Try to compute dy/dx after the gradient tape has expired\n","try:\n","    dy_dx = t.gradient(y, x)  # 6.0\n","    print(dy_dx)\n","except RuntimeError as e:\n","    print(\"The error message is:\")\n","    print(e)"]},{"cell_type":"markdown","metadata":{"id":"xyXPOYJIWL11"},"source":["### Make the gradient tape persistent\n","\n","To ensure that the gradient tape can be used multiple times, set `persistent=True`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1717990759683,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"},"user_tz":240},"id":"P12ExatAPqn6","outputId":"da6de24b-520f-4503-b79f-48a1d03bfbdd"},"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor(108.0, shape=(), dtype=float32)\n"]}],"source":["x = tf.constant(3.0)\n","\n","# Set persistent=True so that we can reuse the tape\n","with tf.GradientTape(persistent=True) as t:\n","    t.watch(x)\n","\n","    # y = x^2\n","    y = x * x\n","\n","    # z = y^2\n","    z = y * y\n","\n","# Compute dz/dx. 4 * x^3 at x = 3\n","dz_dx = t.gradient(z, x)\n","\n","print(dz_dx)"]},{"cell_type":"markdown","metadata":{"id":"63iEZ17xWL11"},"source":["#### Reuse the tape!\n","\n","Let's calculate a second gradient using this persistent tape."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1717990759683,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"},"user_tz":240},"id":"i8S40JENWL12","outputId":"497e1d44-4811-4a11-fd9f-ea27b684e4cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor(6.0, shape=(), dtype=float32)\n"]}],"source":["# We can still compute dy/dx because of the persistent flag.\n","dy_dx = t.gradient(y, x)\n","print(dy_dx)"]},{"cell_type":"markdown","metadata":{"id":"NvCWXXWyWL12"},"source":["As we can see it still works. We can delete the tape variable `t` as it is no longer needed."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZwAtz7uUWL12"},"outputs":[],"source":["# Delete the reference to the tape\n","del t"]},{"cell_type":"markdown","metadata":{"id":"vazvq2VdWL12"},"source":["### Nested Gradient Tapes\n","Now, let's compute a higher-order derivative by nesting `GradientTape`s:\n","\n","#### Proper Indentation for the First Gradient Calculation\n","Ensure that the first gradient calculation of `dy_dx` occurs inside the outer `with` block."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1717990759825,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"},"user_tz":240},"id":"UxNLeFLlP4qU","outputId":"795c797f-be00-4f00-e234-8278135caaa4"},"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor(3.0, shape=(), dtype=float32)\n","tf.Tensor(6.0, shape=(), dtype=float32)\n"]}],"source":["x = tf.Variable(1.0)\n","\n","with tf.GradientTape() as tape_2:\n","    with tf.GradientTape() as tape_1:\n","        y = x * x * x\n","\n","    # The first gradient calculation should occur at least within the outer with block\n","    dy_dx = tape_1.gradient(y, x)\n","d2y_dx2 = tape_2.gradient(dy_dx, x)\n","\n","print(dy_dx)\n","print(d2y_dx2)"]},{"cell_type":"markdown","metadata":{"id":"fnuo-2VzWL13"},"source":["The first gradient calculation can also be placed inside the inner `with` block."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1717990759825,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"},"user_tz":240},"id":"42Rufz-WWL13","outputId":"efa14d49-09fa-42b6-c691-d30d9382774d"},"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor(3.0, shape=(), dtype=float32)\n","tf.Tensor(6.0, shape=(), dtype=float32)\n"]}],"source":["x = tf.Variable(1.0)\n","\n","with tf.GradientTape() as tape_2:\n","    with tf.GradientTape() as tape_1:\n","        y = x * x * x\n","\n","        # The first gradient calculation can also be within the inner with block\n","        dy_dx = tape_1.gradient(y, x)\n","d2y_dx2 = tape_2.gradient(dy_dx, x)\n","\n","print(dy_dx)\n","print(d2y_dx2)"]},{"cell_type":"markdown","metadata":{"id":"kWxqGEV2WL13"},"source":["#### Where not to indent the first gradient calculation\n","If the first gradient calculation is OUTSIDE of the outer `with` block, it won't persist for the second gradient calculation."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":150,"status":"ok","timestamp":1717990759973,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"},"user_tz":240},"id":"nphmJHefWL13","outputId":"a36a194e-926e-4507-d55f-06b1ffe39e1e"},"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor(3.0, shape=(), dtype=float32)\n","None\n"]}],"source":["x = tf.Variable(1.0)\n","\n","with tf.GradientTape() as tape_2:\n","    with tf.GradientTape() as tape_1:\n","        y = x * x * x\n","\n","# The first gradient call is outside the outer with block so the tape will expire after this\n","dy_dx = tape_1.gradient(y, x)\n","\n","# The tape is now expired and the gradient output will be `None`\n","d2y_dx2 = tape_2.gradient(dy_dx, x)\n","\n","print(dy_dx)\n","print(d2y_dx2)"]},{"cell_type":"markdown","metadata":{"id":"RMsOCmedWL14"},"source":["Notice how the `d2y_dx2` calculation is now `None`. The tape has expired. Another thing to note is that setting `persistent=True` for both gradient tapes won't resolve this issue."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":148,"status":"ok","timestamp":1717990760120,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"},"user_tz":240},"id":"F0pBKJZuWL14","outputId":"46aeaa15-f613-49b7-ca40-b781d5a14abb"},"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor(3.0, shape=(), dtype=float32)\n","None\n"]}],"source":["x = tf.Variable(1.0)\n","\n","# Setting persistent=True still won't work\n","with tf.GradientTape(persistent=True) as tape_2:\n","    # Setting persistent=True still won't work\n","    with tf.GradientTape(persistent=True) as tape_1:\n","        y = x * x * x\n","\n","# The first gradient call is outside the outer with block so the tape will expire after this\n","dy_dx = tape_1.gradient(y, x)\n","\n","# The output will be `None`\n","d2y_dx2 = tape_2.gradient(dy_dx, x)\n","\n","print(dy_dx)\n","print(d2y_dx2)"]},{"cell_type":"markdown","metadata":{"id":"3eG1FPv3WL14"},"source":["### Proper indentation for the second gradient calculation\n","\n","The second gradient calculation `d2y_dx2` can be indented as much as the first calculation of `dy_dx` but not more."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1717990760120,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"},"user_tz":240},"id":"Vy-yno-OWL14","outputId":"129e03a5-5123-421d-b477-9085ff83439f"},"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor(3.0, shape=(), dtype=float32)\n","tf.Tensor(6.0, shape=(), dtype=float32)\n"]}],"source":["x = tf.Variable(1.0)\n","\n","with tf.GradientTape() as tape_2:\n","    with tf.GradientTape() as tape_1:\n","        y = x * x * x\n","\n","        dy_dx = tape_1.gradient(y, x)\n","\n","        # This is acceptable\n","        d2y_dx2 = tape_2.gradient(dy_dx, x)\n","\n","print(dy_dx)\n","print(d2y_dx2)"]},{"cell_type":"markdown","metadata":{"id":"iNh1QPT0WL14"},"source":["This is also acceptable"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1717990760120,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"},"user_tz":240},"id":"ax0_H3jmWL14","outputId":"45e88872-b4b6-4535-d6fa-1c68a9068327"},"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor(3.0, shape=(), dtype=float32)\n","tf.Tensor(6.0, shape=(), dtype=float32)\n"]}],"source":["x = tf.Variable(1.0)\n","\n","with tf.GradientTape() as tape_2:\n","    with tf.GradientTape() as tape_1:\n","        y = x * x * x\n","\n","        dy_dx = tape_1.gradient(y, x)\n","\n","    # This is also acceptable\n","    d2y_dx2 = tape_2.gradient(dy_dx, x)\n","\n","print(dy_dx)\n","print(d2y_dx2)"]},{"cell_type":"markdown","metadata":{"id":"rKe10DGMWL15"},"source":["This is also acceptable"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1717990760121,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"},"user_tz":240},"id":"817kBesyWL15","outputId":"8c1a12b0-e41f-41a0-e7ad-fa3fecb9d081"},"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor(3.0, shape=(), dtype=float32)\n","tf.Tensor(6.0, shape=(), dtype=float32)\n"]}],"source":["x = tf.Variable(1.0)\n","\n","with tf.GradientTape() as tape_2:\n","    with tf.GradientTape() as tape_1:\n","        y = x * x * x\n","\n","        dy_dx = tape_1.gradient(y, x)\n","\n","# This is also acceptable\n","d2y_dx2 = tape_2.gradient(dy_dx, x)\n","\n","print(dy_dx)\n","print(d2y_dx2)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}
