{"cells":[{"cell_type":"markdown","metadata":{"id":"S6EFjd6kdted"},"source":["# Creating a Custom Model ─ ResNet\n","\n","In this notebook, we'll delve deeper into Model subclassing by constructing a more intricate architecture known as a Residual Network (ResNet). Residual Networks revolutionized deep learning by introducing skip connections that allow gradients to flow through the network without degradation, significantly easing the training of very deep networks.\n","\n","Residual Networks utilize skip connections, or shortcuts to jump over some layers. Typical ResNet architectures involve several blocks of layers where each block has a skip connection that bypasses one or more layers. <br></br>\n","\n","Steps for Implementing ResNet as a Subclassed Model\n","1. **Model Subclassing:** To manage the complexity inherent in Residual Networks and to enhance code reusability, we'll define our ResNet architecture by subclassing the Model class from Keras. This approach provides a structured way to encapsulate the network's behavior, keeping the code organized and modular.\n","2. **Building Blocks:** Within our ResNet class, we can define separate methods or sub-classes for different blocks of layers. Each block can have its own skip connections and sequence of layers. This modular block design makes it easier to experiment with different layer configurations and to scale the architecture up or down.\n","3. **Utilizing Keras Functionalities:** By inheriting from the Model class, our custom ResNet will inherit all the powerful functionalities of Keras models, including `compile()`, `fit()`, `evaluate()`, and `predict()` methods. This integration allows for a seamless training, evaluation, and application of the deep learning models. <br></br>\n","\n","\n","##### **Advantages of Model Subclassing for Complex Architectures**\n","- **Flexibility**: Subclassing provides the flexibility to implement models that have complex, non-linear topologies with varying layer connections, such as loops, branches, and multi-output configurations.\n","- **Clarity and Modularity**: Organizing the ResNet into a class with methods for each block of layers keeps the code clean and understandable. It also simplifies the process of modifying the architecture or adapting it to different tasks.  <br></br>\n","\n","Building a Residual Network using model subclassing is an excellent way to practice designing advanced neural network architectures. It not only helps in understanding the flow and benefits of gradients within deep networks but also prepares for implementing and managing other sophisticated models in the future projects."]},{"cell_type":"markdown","metadata":{"id":"Lteibds3dteg"},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CmI9MQA6Z72_"},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_datasets as tfds\n","from tensorflow.keras.layers import Layer"]},{"cell_type":"markdown","metadata":{"id":"qIQDD51Ndtei"},"source":["## Implement Model subclasses\n","\n","One of the foundational elements we'll focus on is the Identity Block. This block is crucial for understanding how skip connections work within the architecture to facilitate easier training of deep networks. These skip connections essentially allow the network to learn an identity function that ensures the main path's activations are not altered but are instead incrementally adjusted, which helps in preventing the vanishing gradient problem in deep networks.\n","\n","\n","- **Class Structure:** The Identity Block will be implemented as a subclass of the Model class from Keras. This structure will allow the Identity Block to integrate seamlessly with other Keras functionalities and to be reusable across different parts of the network or in different models.\n","- **Initialization Method (`__init__()`):** Within the initialization method, we will define all the necessary components of the Identity Block. This typically includes convolutional layers, batch normalization layers, and activation layers. Each component is initialized and configured here, ready to be connected in the `call()` method.\n","- **Forward Pass (`call()`):** The call method is where the actual data flow through the block is defined. Inputs pass through the convolutional and normalization layers, and the result is added back to the block's input through a skip connection. This method must handle both the transformation of the input through the block’s main path and the merging of this output with the original input using the `add()` operation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-FIkYUttchv5"},"outputs":[],"source":["class IdentityBlock(tf.keras.Model):\n","    def __init__(self, filters, kernel_size):\n","        super(IdentityBlock, self).__init__(name='')\n","\n","        self.conv1 = tf.keras.layers.Conv2D(filters, kernel_size, padding='same')\n","        self.bn1 = tf.keras.layers.BatchNormalization()\n","\n","        self.conv2 = tf.keras.layers.Conv2D(filters, kernel_size, padding='same')\n","        self.bn2 = tf.keras.layers.BatchNormalization()\n","\n","        self.act = tf.keras.layers.Activation('relu')\n","        self.add = tf.keras.layers.Add()\n","\n","    def call(self, input_tensor):\n","        x = self.conv1(input_tensor)\n","        x = self.bn1(x)\n","        x = self.act(x)\n","\n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","\n","        x = self.add([x, input_tensor])\n","        x = self.act(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"BsGq3tp2dtej"},"source":["With the `Identity Block` implemented, the next step is to build the full Residual Network (ResNet) model. This involves strategically placing our custom Identity Blocks within the network to leverage their benefits throughout the model's depth. We will follow the following steps to build full ResNet Model :\n","\n","1. **Setting Up the Initial Layers:** The ResNet model begins with standard layers that prepare the input for deeper processing. This includes a convolutional layer followed by batch normalization and a ReLU activation to introduce non-linearity. A max pooling layer is then used to reduce the spatial dimensions of the output, condensing the feature maps and reducing computational overhead in deeper layers.\n","2. **Incorporating Identity Blocks:** The core of the ResNet model involves placing Identity Blocks in sequence. In this model, we call the IdentityBlock class twice. Each instance of IdentityBlock processes the data sequentially, applying its internal layers and skip connections to maintain a robust flow of gradients.\n","3. **Completing the Network:** After the data passes through the Identity Blocks, it is pooled globally to reduce each feature map to a single number, effectively summarizing the features extracted by the network. Finally, a dense layer with a softmax activation function classifies the input into the desired number of classes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YnMkmeecxw28"},"outputs":[],"source":["class ResNet(tf.keras.Model):\n","    def __init__(self, num_classes):\n","        super(ResNet, self).__init__()\n","        self.conv = tf.keras.layers.Conv2D(64, 7, padding='same')\n","        self.bn = tf.keras.layers.BatchNormalization()\n","        self.act = tf.keras.layers.Activation('relu')\n","        self.max_pool = tf.keras.layers.MaxPool2D((3, 3))\n","\n","        # Use the Identity blocks that we just defined\n","        self.id1a = IdentityBlock(64, 3)\n","        self.id1b = IdentityBlock(64, 3)\n","\n","        self.global_pool = tf.keras.layers.GlobalAveragePooling2D()\n","        self.classifier = tf.keras.layers.Dense(num_classes, activation='softmax')\n","\n","    def call(self, inputs):\n","        x = self.conv(inputs)\n","        x = self.bn(x)\n","        x = self.act(x)\n","        x = self.max_pool(x)\n","\n","        # insert the identity blocks in the middle of the network\n","        x = self.id1a(x)\n","        x = self.id1b(x)\n","\n","        x = self.global_pool(x)\n","        return self.classifier(x)"]},{"cell_type":"markdown","metadata":{"id":"O6xwhfhBdtek"},"source":["## Training the Model\n","\n","With our ResNet model defined as a subclass of the Model class from Keras, we are now well-equipped to leverage the full suite of functionalities that Keras offers. This structure not only supports a streamlined training process but also allows for easy model serialization and evaluation.<br></br>\n","\n","#### **Key Features Leveraged from Keras:**\n","1. **Training:** We can train the model using the standard `.fit()` method, which handles everything from forward propagation to backpropagation and updating the model weights.\n","2. **Serialization:** Keras provides methods for saving and loading the model, which is crucial for deployment or continuing training later.\n","3. **Evaluation:** After training, we can use `.evaluate()` to test the model’s performance on new, unseen data, providing metrics such as loss and accuracy.\n","\n","\n","Once the ResNet model class is defined, we can instantiate and train it just like any other Keras model."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6dMHKPz_dIc8","outputId":"bd39458a-187e-4767-d345-8068231b4847"},"outputs":[{"name":"stdout","output_type":"stream","text":["1875/1875 [==============================] - 16s 7ms/step - loss: 0.1336 - accuracy: 0.9638\n"]},{"data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f719013e210>"]},"execution_count":4,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["# utility function to normalize the images and return (image, label) pairs.\n","def preprocess(features):\n","    return tf.cast(features['image'], tf.float32) / 255., features['label']\n","\n","# create a ResNet instance with 10 output units for MNIST\n","resnet = ResNet(10)\n","resnet.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# load and preprocess the dataset\n","dataset = tfds.load('mnist', split=tfds.Split.TRAIN)\n","dataset = dataset.map(preprocess).batch(32)\n","\n","# train the model\n","resnet.fit(dataset, epochs=1)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":0}
