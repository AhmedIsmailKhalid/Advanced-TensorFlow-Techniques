{"cells":[{"cell_type":"markdown","metadata":{"id":"CHLBJlo0HTNa"},"source":["# Implementing a Custom Quadratic Layer"]},{"cell_type":"markdown","metadata":{"id":"fMuH22GjHTNf"},"source":["In this notebook, we will be focusing on creating a custom quadratic layer. This layer will perform computations based on the quadratic equation : $y = ax^2 + bx + c $ where $a$, $b$ and $c$ are the parameters that the layer will learn during training.\n","\n","This specialized layer will be integrated into a model designed to work with the MNIST dataset. This exercise will give us the opportunity to explore more deeply how custom layers can be used to enhance model performance on real-world tasks. Let‚Äôs dive in and start building this exciting feature!"]},{"cell_type":"markdown","metadata":{"id":"PhqGNamtHTNg"},"source":["### Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x8-8SmTLHTNg"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Layer"]},{"cell_type":"markdown","metadata":{"id":"cpCyuIYaHTNi"},"source":["### Define the quadratic layer\n","In this section, we'll build a custom layer called SimpleQuadratic to perform computations based on the quadratic formula $y=ax^2+bx+c$. This layer will have three trainable parameters: $ùëé$, $ùëè$, and $ùëê$, and will optionally allow an activation function to be applied to the output. This design provides a flexible way to integrate polynomial computations into a neural network. $ax^2 + bx + c$. We have to make sure it can also accept an activation function.\n","\n","#### **Initialization (`__init__`)**\n","1. **Inherit from Base Class:** Begin by calling the initialization method of the base class, Layer, to ensure all underlying mechanisms are set up correctly.\n","2. **Units:** Define a units attribute that specifies the dimensionality of the output space (i.e., the number of output neurons).\n","3. **Activation Function:** Allow an activation function to be specified as a string, which can be converted into a TensorFlow object using tf.keras.activations.get(). This flexibility lets us apply any standard activation function to the output of the quadratic computations.\n","\n","#### **Building the Layer (`build`)**\n","1. **Parameter Initialization:**\n","    - $a$: Initialize this weight using a normal distribution. Its shape should align with the last dimension of `input_shape` to ensure it can be appropriately multiplied with $x^2$\n","    - $b$: Similar to $a$, initialize using a normal distribution. while ensuring the shape matches so that $x$ can be multiplied with $b$.\n","    - $c$: Initialize this bias term with zeros. The length of $c$ should match the units, representing a bias for each unit.\n","2. **Variables as Trainable:** Mark $a$, $b$, and $c$ as trainable variables to ensure they are adjusted during the training process through backpropagation.\n","\n","#### **Forward Computation (`call`)**\n","1. **Apply the Quadratic Formula:**\n","    - Compute $x^2$ and then perform a matrix multiplication with $a$ (adjust for matrix compatibility).\n","    - Multiply $x$ with $b$ and ensure dimensions are correct for matrix operations.\n","    - Sum $x^2a$. $xb$ and $c$ get the preliminary output of the layer.\n","2. **Activation Function:** If an activation function is specified, apply it to the output of the summation to introduce non-linearity.\n","3. **Output:** Return the final activated value, which is the result of the custom quadratic computations followed by the optional activation.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ga20PttZFXm4"},"outputs":[],"source":["class SimpleQuadratic(Layer):\n","\n","    def __init__(self, units=32, activation=None):\n","        '''Initializes the class and sets up the internal variables'''\n","        super(SimpleQuadratic, self).__init__()\n","        self.units = units\n","        self.activation = tf.keras.activations.get(activation)\n","\n","    def build(self, input_shape):\n","        '''Create the state of the layer (weights)'''\n","        # a and b should be initialized with random normal, c (or the bias) with zeros and are going to be set as trainable.\n","        self.a = tf.Variable(name='a',\n","                             initial_value=tf.random_normal_initializer()(shape=(input_shape[-1], self.units),\n","                                                                          dtype= 'float32'), trainable = True)\n","        self.b = tf.Variable(name='b',\n","                             initial_value=tf.random_normal_initializer()(shape=(input_shape[-1], self.units),\n","                                                                          dtype='float32'), trainable=True)\n","        '''self.c = tf.Variable(name='c',\n","                             initial_value=tf.zeros_initializer(shape=(self.units, ),\n","                                                                          dtype='float32'), trainable=True)'''\n","        self.c = tf.Variable(name='b',\n","                             initial_value=tf.zeros_initializer()(shape=(self.units, ),\n","                                                                          dtype='float32'), trainable=True)\n","\n","    def call(self, inputs):\n","        '''Defines the computation from inputs to outputs'''\n","        # Use self.activation() to get the final output\n","        return self.activation(tf.matmul(tf.math.square(inputs), self.a) + tf.matmul(inputs, self.b) + self.c)"]},{"cell_type":"markdown","metadata":{"id":"zcx2gSmLHTNl"},"source":["Now that we have implemented our custom `SimpleQuadratic` layer, the next step is to train a model incorporating this layer. Training the model will allow us to observe how well the quadratic computations performed by the layer contribute to the overall task, such as classifying images or predicting outcomes based on input data."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":88241,"status":"ok","timestamp":1717963300380,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"},"user_tz":240},"id":"14tl1CluExjJ","outputId":"15cb8637-3fe0-4f4c-a5aa-0abf92ceb8fb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","1875/1875 [==============================] - 16s 8ms/step - loss: 0.2640 - accuracy: 0.9218\n","Epoch 2/5\n","1875/1875 [==============================] - 12s 6ms/step - loss: 0.1329 - accuracy: 0.9601\n","Epoch 3/5\n","1875/1875 [==============================] - 12s 6ms/step - loss: 0.0994 - accuracy: 0.9690\n","Epoch 4/5\n","1875/1875 [==============================] - 12s 6ms/step - loss: 0.0840 - accuracy: 0.9735\n","Epoch 5/5\n","1875/1875 [==============================] - 10s 5ms/step - loss: 0.0722 - accuracy: 0.9766\n","313/313 [==============================] - 1s 2ms/step - loss: 0.0773 - accuracy: 0.9776\n"]},{"data":{"text/plain":["[0.0772533193230629, 0.9775999784469604]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["mnist = tf.keras.datasets.mnist\n","\n","(x_train, y_train),(x_test, y_test) = mnist.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","\n","model = tf.keras.models.Sequential([\n","  tf.keras.layers.Flatten(input_shape=(28, 28)),\n","  SimpleQuadratic(128, activation='relu'),\n","  tf.keras.layers.Dropout(0.2),\n","  tf.keras.layers.Dense(10, activation='softmax')\n","])\n","\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","model.fit(x_train, y_train, epochs=5)\n","model.evaluate(x_test, y_test)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}
