{"cells":[{"cell_type":"markdown","metadata":{"id":"rmzwPwcssrVE"},"source":["# GANs on CelebA Dataset\n","\n","In this notebook, we will look at a GAN trained on the [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset. Given the resource-intensive nature of this task, we will use a TPU and a distributed strategy to train the network. Once the training is done, we will see a gif showcasing new faces generated by the trained model."]},{"cell_type":"markdown","metadata":{"id":"kUq5cRGbs17s"},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30183,"status":"ok","timestamp":1718226597910,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"},"user_tz":240},"id":"lwhm4tTZZF21","outputId":"287f1c31-056d-4698-f031-d0dfaf29b30f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting tensorflow-addons\n","  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.1)\n","Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n","  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n","Installing collected packages: typeguard, tensorflow-addons\n","Successfully installed tensorflow-addons-0.23.0 typeguard-2.13.3\n"]}],"source":["# Install tensorflow_addons\n","!pip install -U tensorflow-addons"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15939,"status":"ok","timestamp":1718226613846,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"},"user_tz":240},"id":"udWFxlOvYPgh","outputId":"2f3f29d9-6cd1-4465-d37d-3c18306d7fe4"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n","\n","TensorFlow Addons (TFA) has ended development and introduction of new features.\n","TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n","Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n","\n","For more information see: https://github.com/tensorflow/addons/issues/2807 \n","\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.13.0 and strictly below 2.16.0 (nightly versions are not supported). \n"," The versions of TensorFlow you are currently using is 2.12.0 and is not supported. \n","Some things might work, some things might not.\n","If you were to encounter a bug, do not file an issue.\n","If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n","You can find the compatibility matrix in TensorFlow Addon's readme:\n","https://github.com/tensorflow/addons\n","  warnings.warn(\n"]}],"source":["import tensorflow as tf\n","import tensorflow_datasets as tfds\n","from tensorflow.keras.utils import plot_model\n","from tensorflow.keras import layers\n","import tensorflow_addons as tfa\n","\n","import os\n","import zipfile\n","import glob\n","import urllib.request\n","from enum import Enum\n","from tqdm import tqdm\n","from functools import partial\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from PIL import Image\n","from IPython.display import display\n","from IPython.display import Image as IpyImage\n","import imageio\n","import cv2"]},{"cell_type":"markdown","metadata":{"id":"zkpzuUi7vpsh"},"source":["## Setup TPU\n","\n","We will use a TPU and its corresponding [distribution strategy](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy) to speed up the training process. In addition this let's also define some helper functions as well."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12093,"status":"ok","timestamp":1718226625936,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"},"user_tz":240},"id":"rK_RTnYOYmKi","outputId":"b916bd96-84e0-414f-b047-8d79ad1c44d0"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"]}],"source":["tpu_grpc_url = \"grpc://\" + os.environ[\"COLAB_TPU_ADDR\"]\n","tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu_grpc_url)\n","tf.config.experimental_connect_to_cluster(tpu_cluster_resolver)\n","tf.tpu.experimental.initialize_tpu_system(tpu_cluster_resolver)\n","strategy = tf.distribute.experimental.TPUStrategy(tpu_cluster_resolver)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v9qrAyLDYpxJ"},"outputs":[],"source":["class Reduction(Enum):\n","    NONE = 0\n","    SUM = 1\n","    MEAN = 2\n","    CONCAT = 3\n","\n","def distributed(*reduction_flags):\n","    def _decorator(fun):\n","        def per_replica_reduction(z, flag):\n","            if flag == Reduction.NONE:\n","                return z\n","            elif flag == Reduction.SUM:\n","                return strategy.reduce(tf.distribute.ReduceOp.SUM, z, axis=None)\n","            elif flag == Reduction.MEAN:\n","                return strategy.reduce(tf.distribute.ReduceOp.MEAN, z, axis=None)\n","            elif flag == Reduction.CONCAT:\n","                z_list = strategy.experimental_local_results(z)\n","                return tf.concat(z_list, axis=0)\n","            else:\n","                raise NotImplementedError()\n","\n","        @tf.function\n","        def _decorated_fun(*args, **kwargs):\n","            fun_result = strategy.run(fun, args=args, kwargs=kwargs)\n","            if len(reduction_flags) == 0:\n","                assert fun_result is None\n","                return\n","            elif len(reduction_flags) == 1:\n","                assert type(fun_result) is not tuple and fun_redult is not None\n","                return per_replica_reduction(fun_result, *reduction_flags)\n","            else:\n","                assert type(fun_result) is tuple\n","                return tuple((per_replica_reduction(fr, rf) for fr, rf in zip(fun_result, reduction_flags)))\n","        return _decorated_fun\n","    return _decorator"]},{"cell_type":"markdown","metadata":{"id":"bbhebu8EvsCz"},"source":["## Download and Prepare the Dataset\n","\n","Next, we will fetch the celebrity faces dataset. A copy of the data has been hosted in a Google Drive, but please note that the filesize is around 1GB, so it will take some time to download."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bqxdEl0wYa13"},"outputs":[],"source":["# Make a data directory\n","try:\n","  os.mkdir('/tmp/celeb')\n","except OSError:\n","  pass\n","\n","# Download the dataset archive\n","data_url = \"https://storage.googleapis.com/learning-datasets/Resources/archive.zip\"\n","data_file_name = \"archive.zip\"\n","download_dir = '/tmp/celeb/'\n","urllib.request.urlretrieve(data_url, data_file_name)\n","\n","# Extract the zipped file\n","zip_ref = zipfile.ZipFile(data_file_name, 'r')\n","zip_ref.extractall(download_dir)\n","zip_ref.close()"]},{"cell_type":"markdown","metadata":{"id":"VAz0njNjq98x"},"source":["We will then prepare the dataset. The preprocessing steps include cropping and transforming the pixel values to the range `[-1, 1]`. Training batches are then prepared so they can be fed into the model later."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":304182,"status":"ok","timestamp":1718227345354,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"},"user_tz":240},"id":"Qb_FccO3urG8","outputId":"9ef40a27-94c1-4380-94b6-ebc0649c6edc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating Images\n"]},{"name":"stderr","output_type":"stream","text":["202599it [04:36, 731.52it/s]\n"]}],"source":["def load_celeba(batch_size, resize=64, crop_size=128):\n","  \"\"\"Creates batches of preprocessed images from the JPG files\n","  Args:\n","    batch_size - batch size\n","    resize - size in pixels to resize the images\n","    crop_size - size to crop from the image\n","\n","  Returns:\n","    prepared dataset\n","  \"\"\"\n","  # Initialize zero-filled array equal to the size of the dataset\n","  image_paths = sorted(glob.glob(\"/tmp/celeb/img_align_celeba/img_align_celeba/*.jpg\"))\n","  images = np.zeros((len(image_paths), resize, resize, 3), np.uint8)\n","  print(\"Creating Images\")\n","\n","  # Crop and resize the raw images then put into the array\n","  for i, path in tqdm(enumerate(image_paths)):\n","    with Image.open(path) as img:\n","      left = (img.size[0] - crop_size) // 2\n","      top = (img.size[1] - crop_size) // 2\n","      right = left + crop_size\n","      bottom = top + crop_size\n","      img = img.crop((left, top, right, bottom))\n","      img = img.resize((resize, resize), Image.LANCZOS)\n","      images[i] = np.asarray(img, np.uint8)\n","\n","  # Split the images array into two\n","  split_n = images.shape[0] // 2\n","  images1, images2 = images[:split_n], images[split_n:2 * split_n]\n","  del images\n","\n","  # Preprocessing function to convert the pixel values into the range [-1,1]\n","  def preprocess(img):\n","      x = tf.cast(img, tf.float32) / 127.5 - 1.0\n","      return x\n","\n","  # Use the preprocessing function on the arrays and create batches\n","  dataset = tf.data.Dataset.from_tensor_slices((images1, images2))\n","  dataset = dataset.map(\n","      lambda x1, x2: (preprocess(x1), preprocess(x2))\n","  ).shuffle(4096).batch(batch_size, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n","\n","  return dataset\n","\n","# Use the function above to load and prepare the dataset\n","batch_size = 8\n","batch_size = batch_size * strategy.num_replicas_in_sync\n","dataset = load_celeba(batch_size)\n","out_dir = \"celeba_out\""]},{"cell_type":"markdown","metadata":{"id":"QpLfJuXfuVPk"},"source":["## Build the Model\n","\n","Next, we will build the generator and discriminator. The code in this notebook is generalized to make it easy to reconfigure, such as choosing the type of normalization. Due to this, we will notice a lot of extra code, mostly related to gradient penalty. We can ignore those and set the defaults to reflect the architecture shown in class.\n","\n","We can try the other settings once we've gone through these defaults. Additional modes made available are based on DRAGAN and WGAN-GP. We can read about them [here](https://arxiv.org/abs/1705.07215) and [here](https://arxiv.org/abs/1704.00028v3). These settings are reconfigured using the utilities below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QvsgWnBEYvhO"},"outputs":[],"source":["# Utilities\n","\n","def _get_norm_layer(norm):\n","    if norm == 'NA':\n","        return lambda: lambda x: x\n","    elif norm == 'batch_normalization':\n","        return layers.BatchNormalization\n","    elif norm == 'instance_normalization':\n","        return tfa.layers.InstanceNormalization\n","    elif norm == 'layer_normalization':\n","        return layers.LayerNormalization\n","\n","\n","def get_initializers():\n","    return (tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02), # conv initializer\n","            tf.keras.initializers.RandomNormal(mean=1.0, stddev=0.02)) # bn gamma initializer\n","\n","\n","def gradient_penalty(f, real, fake, mode):\n","    def _gradient_penalty(f, real, fake=None):\n","        def _interpolate(a, b=None):\n","            if b is None:   # interpolation in DRAGAN\n","                beta = tf.random.uniform(shape=tf.shape(a), minval=0., maxval=1.)\n","                b = a + 0.5 * tf.math.reduce_std(a) * beta\n","            shape = [tf.shape(a)[0]] + [1] * (a.shape.ndims - 1)\n","            alpha = tf.random.uniform(shape=shape, minval=0., maxval=1.)\n","            inter = a + alpha * (b - a)\n","            inter.set_shape(a.shape)\n","            return inter\n","\n","        x = _interpolate(real, fake)\n","        with tf.GradientTape() as t:\n","            t.watch(x)\n","            pred = f(x)\n","        grad = t.gradient(pred, x)\n","        norm = tf.norm(tf.reshape(grad, [tf.shape(grad)[0], -1]), axis=1)\n","        gp = tf.reduce_mean((norm - 1.)**2)\n","\n","        return gp\n","\n","    if mode == 'none':\n","        gp = tf.constant(0, dtype=real.dtype)\n","    elif mode == 'dragan':\n","        gp = _gradient_penalty(f, real)\n","    elif mode == 'wgan-gp':\n","        gp = _gradient_penalty(f, real, fake)\n","\n","    return gp"]},{"cell_type":"markdown","metadata":{"id":"gn2D6EuKzasq"},"source":["### Generator\n","\n","We will first define the generator layers. Again, we will notice some extra code, but the default will follow the architecture in class. Like the DCGAN we previously built in the `Introduction to DCGANs` notebook, the model here primarily uses blocks containing Conv2D, BatchNormalization, and ReLU layers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7noZmy02ysCS"},"outputs":[],"source":["def create_generator(input_shape=(1, 1, 128),\n","                    output_channels=3,\n","                    dim=64,\n","                    n_upsamplings=4,\n","                    norm='batch_normalization',\n","                    name='generator'):\n","\n","    Normalization = _get_norm_layer(norm)\n","    conv_initializer, bn_gamma_initializer = get_initializers()\n","\n","    # 0\n","    x = inputs = tf.keras.Input(shape=input_shape)\n","\n","    # 1: 1x1 -> 4x4\n","    dimensions = min(dim * 2 ** (n_upsamplings - 1), dim * 8)\n","    x = layers.Conv2DTranspose(\n","        dimensions, 4, strides=1, padding='valid', use_bias=False,\n","        # kernel_initializer=conv_initializer\n","    )(x)\n","    x = Normalization(\n","        # gamma_initializer=bn_gamma_initializer\n","        )(x)\n","    x = layers.ReLU()(x)\n","\n","    # 2: Upsamplings, 4x4 -> 8x8 -> 16x16 -> ...\n","    for i in range(n_upsamplings - 1):\n","        dimensions = min(dim * 2 ** (n_upsamplings - 2 - i), dim * 8)\n","        x = layers.Conv2DTranspose(\n","            dimensions, 4, strides=2, padding='same', use_bias=False,\n","            # kernel_initializer=conv_initializer\n","            )(x)\n","        x = Normalization(\n","            # gamma_initializer=bn_gamma_initializer\n","            )(x)\n","        x = layers.ReLU()(x)\n","\n","    x = layers.Conv2DTranspose(\n","        output_channels, 4, strides=2, padding='same',\n","        # kernel_initializer=conv_initializer\n","    )(x)\n","\n","    outputs = layers.Activation('tanh')(x)\n","\n","    return tf.keras.Model(inputs=inputs, outputs=outputs, name=name)"]},{"cell_type":"markdown","metadata":{"id":"3T07PeND0vny"},"source":["### Discriminator\n","\n","The discriminator will utilize strided convolutions to reduce the dimensionality of the features. These layers will be connected to a [LeakyReLU](https://keras.io/api/layers/activation_layers/leaky_relu/) activation for effective feature extraction."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dMrJ1-JYnOEF"},"outputs":[],"source":["def create_discriminator(input_shape=(64, 64, 3),\n","                        dim=64,\n","                        n_downsamplings=4,\n","                        norm='batch_normalization',\n","                        name='discriminator'):\n","    Normalization = _get_norm_layer(norm)\n","    conv_initializer, bn_gamma_initializer = get_initializers()\n","\n","    # 0\n","    x = inputs = tf.keras.Input(shape=input_shape)\n","\n","    # 1: Downsamplings, ... -> 16x16 -> 8x8 -> 4x4\n","    x = layers.Conv2D(dim, 4, strides=2, padding='same',\n","                      # kernel_initializer=conv_initializer\n","                      )(x)\n","    x = layers.LeakyReLU(alpha=0.2)(x)\n","\n","    for i in range(n_downsamplings - 1):\n","        dimensions = min(dim * 2 ** (i + 1), dim * 8)\n","        x = layers.Conv2D(dimensions, 4, strides=2, padding='same', use_bias=False,\n","                          # kernel_initializer=conv_initializer\n","                          )(x)\n","        x = Normalization(\n","            # gamma_initializer=bn_gamma_initializer\n","            )(x)\n","        x = layers.LeakyReLU(alpha=0.2)(x)\n","\n","    # 2: Logit\n","    outputs = layers.Conv2D(1, 4, strides=1, padding='valid',\n","                            # kernel_initializer=conv_initializer\n","                            )(x)\n","\n","    return tf.keras.Model(inputs=inputs, outputs=outputs, name=name)"]},{"cell_type":"markdown","metadata":{"id":"QT7qi7j21VR3"},"source":["With the layers for the generator and discriminator defined, we can now create the models and set them up for training."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4151,"status":"ok","timestamp":1718227349504,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"},"user_tz":240},"id":"9javUztztr9t","outputId":"1a17ca13-deb0-4c97-a28c-291840cbb00b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"generator\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 1, 1, 128)]       0         \n","                                                                 \n"," conv2d_transpose (Conv2DTra  (None, 4, 4, 512)        1048576   \n"," nspose)                                                         \n","                                                                 \n"," batch_normalization (BatchN  (None, 4, 4, 512)        2048      \n"," ormalization)                                                   \n","                                                                 \n"," re_lu (ReLU)                (None, 4, 4, 512)         0         \n","                                                                 \n"," conv2d_transpose_1 (Conv2DT  (None, 8, 8, 256)        2097152   \n"," ranspose)                                                       \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 8, 8, 256)        1024      \n"," hNormalization)                                                 \n","                                                                 \n"," re_lu_1 (ReLU)              (None, 8, 8, 256)         0         \n","                                                                 \n"," conv2d_transpose_2 (Conv2DT  (None, 16, 16, 128)      524288    \n"," ranspose)                                                       \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 16, 16, 128)      512       \n"," hNormalization)                                                 \n","                                                                 \n"," re_lu_2 (ReLU)              (None, 16, 16, 128)       0         \n","                                                                 \n"," conv2d_transpose_3 (Conv2DT  (None, 32, 32, 64)       131072    \n"," ranspose)                                                       \n","                                                                 \n"," batch_normalization_3 (Batc  (None, 32, 32, 64)       256       \n"," hNormalization)                                                 \n","                                                                 \n"," re_lu_3 (ReLU)              (None, 32, 32, 64)        0         \n","                                                                 \n"," conv2d_transpose_4 (Conv2DT  (None, 64, 64, 3)        3075      \n"," ranspose)                                                       \n","                                                                 \n"," activation (Activation)     (None, 64, 64, 3)         0         \n","                                                                 \n","=================================================================\n","Total params: 3,808,003\n","Trainable params: 3,806,083\n","Non-trainable params: 1,920\n","_________________________________________________________________\n","Model: \"discriminator\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 64, 64, 3)]       0         \n","                                                                 \n"," conv2d (Conv2D)             (None, 32, 32, 64)        3136      \n","                                                                 \n"," leaky_re_lu (LeakyReLU)     (None, 32, 32, 64)        0         \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 16, 16, 128)       131072    \n","                                                                 \n"," batch_normalization_4 (Batc  (None, 16, 16, 128)      512       \n"," hNormalization)                                                 \n","                                                                 \n"," leaky_re_lu_1 (LeakyReLU)   (None, 16, 16, 128)       0         \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 8, 8, 256)         524288    \n","                                                                 \n"," batch_normalization_5 (Batc  (None, 8, 8, 256)        1024      \n"," hNormalization)                                                 \n","                                                                 \n"," leaky_re_lu_2 (LeakyReLU)   (None, 8, 8, 256)         0         \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 4, 4, 512)         2097152   \n","                                                                 \n"," batch_normalization_6 (Batc  (None, 4, 4, 512)        2048      \n"," hNormalization)                                                 \n","                                                                 \n"," leaky_re_lu_3 (LeakyReLU)   (None, 4, 4, 512)         0         \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 1, 1, 1)           8193      \n","                                                                 \n","=================================================================\n","Total params: 2,767,425\n","Trainable params: 2,765,633\n","Non-trainable params: 1,792\n","_________________________________________________________________\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n","WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"]}],"source":["# Settings\n","resize = 64\n","shape = (resize, resize, 3)\n","z_dim = 128\n","n_G_upsamplings = n_D_downsamplings = 4\n","gradient_penalty_mode = 'none'\n","\n","if gradient_penalty_mode == 'none':\n","  d_norm = 'batch_normalization'\n","elif gradient_penalty_mode in ['dragan', 'wgan-gp']:\n","  # Avoid using BN with GP\n","  d_norm = 'layer_normalization'\n","gradient_penalty_weight = 10.0\n","\n","\n","# Build the GAN\n","with strategy.scope():\n","    # Create the generator model\n","    model_G = create_generator(input_shape=(1, 1, z_dim), output_channels=shape[-1], n_upsamplings=n_G_upsamplings)\n","\n","    # Create the discriminator model\n","    model_D = create_discriminator(input_shape=shape, n_downsamplings=n_D_downsamplings, norm=d_norm)\n","\n","    # Print summaries\n","    model_G.summary()\n","    model_D.summary()\n","\n","    # Set optimizers\n","    param_G = tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n","    param_D = tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n","\n","    # Create distributed dataset\n","    dataset = strategy.experimental_distribute_dataset(dataset)\n","\n","    # Set the loss function\n","    loss_func = tf.keras.losses.BinaryCrossentropy(\n","        from_logits=True,\n","        reduction=tf.keras.losses.Reduction.NONE\n","    )"]},{"cell_type":"markdown","metadata":{"id":"98EZkGvKwZaP"},"source":["## Training\n","\n","We can now train the model. Before we train the model, let's define some functions for visualizing and saving the images per epoch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nm7tc4s2uWn_"},"outputs":[],"source":["# Helper Functions\n","\n","def make_grid(imgs, nrow, padding=0):\n","    assert imgs.ndim == 4 and nrow > 0\n","\n","    batch, height, width, ch = imgs.shape\n","    n = nrow * (batch // nrow + np.sign(batch % nrow))\n","    ncol = n // nrow\n","    pad = np.zeros((n - batch, height, width, ch), imgs.dtype)\n","    x = np.concatenate([imgs, pad], axis=0)\n","\n","    # Border padding if required\n","    if padding > 0:\n","        x = np.pad(x, ((0, 0), (0, padding), (0, padding), (0, 0)),\n","                   \"constant\", constant_values=(0, 0))\n","        height += padding\n","        width += padding\n","\n","    x = x.reshape(ncol, nrow, height, width, ch)\n","    x = x.transpose([0, 2, 1, 3, 4])  # (ncol, height, nrow, width, ch)\n","    x = x.reshape(height * ncol, width * nrow, ch)\n","\n","    if padding > 0:\n","        x = x[:(height * ncol - padding),:(width * nrow - padding),:]\n","    return x\n","\n","def save_img(imgs, filepath, nrow, padding=0):\n","    grid_img = make_grid(imgs, nrow, padding=padding)\n","    grid_img = ((grid_img + 1.0) * 127.5).astype(np.uint8)\n","    with Image.fromarray(grid_img) as img:\n","        img.save(filepath)"]},{"cell_type":"markdown","metadata":{"id":"8hphQ5P53PQu"},"source":["This function defines the training on a given batch. It does the two-phase training discussed in class:\n","\n","First, we train the discriminator to distinguish between fake and real images.\n","Next, we train the generator to create fake images that will fool the discriminator."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gKNwOru-2_Vb"},"outputs":[],"source":["@distributed(Reduction.SUM, Reduction.SUM, Reduction.CONCAT)\n","def train_on_batch(real_img1, real_img2):\n","    '''Trains the GAN on a given batch'''\n","    # Concatenate the real image inputs\n","    real_img = tf.concat([real_img1, real_img2], axis=0)\n","\n","    # PHASE ONE - train the discriminator\n","    with tf.GradientTape() as d_tape:\n","\n","        # Create noise input\n","        z = tf.random.normal(shape=(real_img.shape[0], 1, 1, z_dim))\n","\n","        # Generate fake images\n","        fake_img = model_G(z)\n","\n","        # Feed the fake images to the discriminator\n","        fake_out = model_D(fake_img)\n","\n","        # Feed the real images to the discriminator\n","        real_out = model_D(real_img)\n","\n","        # Use the loss function to measure how well the discriminator labels fake or real images\n","        d_fake_loss = loss_func(tf.zeros_like(fake_out), fake_out)\n","        d_real_loss = loss_func(tf.ones_like(real_out), real_out)\n","\n","        # Get the total loss\n","        d_loss = (d_fake_loss + d_real_loss)\n","        d_loss = tf.reduce_sum(d_loss) / (batch_size * 2)\n","\n","        # Gradient Penalty (ignore if mode set to `none`)\n","        gp = gradient_penalty(partial(model_D, training=True), real_img, fake_img, mode=gradient_penalty_mode)\n","        gp = gp  / (batch_size * 2)\n","        d_loss = d_loss + gp * gradient_penalty_weight\n","\n","    # Get the gradients\n","    gradients = d_tape.gradient(d_loss, model_D.trainable_variables)\n","\n","    # Update the weights of the discriminator\n","    param_D.apply_gradients(zip(gradients, model_D.trainable_variables))\n","\n","\n","    # PHASE TWO - train the generator\n","    with tf.GradientTape() as g_tape:\n","        # Create noise input\n","        z = tf.random.normal(shape=(real_img.shape[0], 1, 1, z_dim))\n","\n","        # Generate fake images\n","        fake_img = model_G(z)\n","\n","        # Feed fake images to the discriminator\n","        fake_out = model_D(fake_img)\n","\n","        # Use loss function to measure how well the generator is able to trick the discriminator (i.e. model_D should output 1's)\n","        g_loss = loss_func(tf.ones_like(fake_out), fake_out)\n","        g_loss = tf.reduce_sum(g_loss) / (batch_size * 2)\n","\n","    # Get the gradients\n","    gradients = g_tape.gradient(g_loss, model_G.trainable_variables)\n","\n","    # Update the weights of the generator\n","    param_G.apply_gradients(zip(gradients, model_G.trainable_variables))\n","\n","    # Return the losses and fake images for monitoring\n","    return d_loss, g_loss, fake_img"]},{"cell_type":"markdown","metadata":{"id":"ZTn0jGin6ldz"},"source":["This will start the training loop. We set the number of epochs to 10 to reduce the run time for this notebook. This will limit the results of our GAN model The results can be improved by simply increasing the number of epochs, however the trade-off for this is it will take more time.\n","\n","We've set up a progress bar to display the losses per epoch, and there is code as well to print the fake images generated."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Vx2_1MJKOmg1SYukUJgCWmOq8VqX0Cae"},"executionInfo":{"elapsed":846822,"status":"ok","timestamp":1718228317379,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"},"user_tz":240},"id":"snusiGSBtrlU","outputId":"a0e63b3b-2f65-4c33-b797-7cbc510e1103"},"outputs":[{"data":{"text/plain":["Output hidden; open in https://colab.research.google.com to view."]},"metadata":{},"output_type":"display_data"}],"source":["# Generate a batch of noisy input\n","test_z = tf.random.normal(shape=(64, 1, 1, z_dim))\n","\n","# Start loop\n","for epoch in range(10):\n","    with tqdm(dataset) as pbar:\n","        pbar.set_description(f\"[Epoch {epoch}]\")\n","        for step, (X1, X2) in enumerate(pbar):\n","            # Train on the current batch\n","            d_loss, g_loss, fake = train_on_batch(X1, X2)\n","\n","            # Display the losses\n","            pbar.set_postfix({\"g_loss\": g_loss.numpy(), \"d_loss\": d_loss.numpy()})\n","\n","        # Generate fake images\n","        fake_img = model_G(test_z)\n","\n","    # Save output\n","    if not os.path.exists(out_dir):\n","        os.makedirs(out_dir)\n","    file_path = out_dir+f\"/epoch_{epoch:04}.png\"\n","    save_img(fake_img.numpy()[:64], file_path, 8)\n","\n","    # Display gallery of fake faces\n","    if epoch % 1 == 0:\n","        with Image.open(file_path) as img:\n","            plt.imshow(np.asarray(img))\n","            plt.show()"]},{"cell_type":"markdown","metadata":{"id":"NnIhFWClwbmq"},"source":["## Display GIF Sample Results"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":7611,"status":"ok","timestamp":1718228373632,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"},"user_tz":240},"id":"6DiVvDz7d845"},"outputs":[],"source":["imgs = os.listdir('celeba_out')\n","imgs.sort()\n","imgs = [cv2.imread('celeba_out/' + i) for i in imgs]\n","imgs = [cv2.cvtColor(i, cv2.COLOR_BGR2RGB) for i in imgs]\n","imageio.mimsave('anim.gif', imgs, fps=2)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":529,"output_embedded_package_id":"1R1uApoFXCxg48_nZkKXNKoj4nAbb3pLB"},"executionInfo":{"elapsed":2571,"status":"ok","timestamp":1718228376201,"user":{"displayName":"Ahmed Ismail Khalid","userId":"15798079041176304147"},"user_tz":240},"id":"JmllPCyfeZsN","outputId":"5b1cbf92-d015-4e02-81b2-a4e9cb63f489"},"outputs":[{"data":{"text/plain":["Output hidden; open in https://colab.research.google.com to view."]},"metadata":{},"output_type":"display_data"}],"source":["path=\"anim.gif\"\n","\n","with open(path,'rb') as f:\n","    display(IpyImage(data=f.read(), format='png'))"]}],"metadata":{"accelerator":"TPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
